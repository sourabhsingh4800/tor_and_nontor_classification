{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ePL9bLIxWp7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Read in data and display first 5 rows\n",
        "#for random forest , KNN , extratrees, logistic regression\n",
        "pd.set_option('display.max_columns', None)\n",
        "features = pd.read_csv('/content/tor_nontor.csv')\n",
        "features.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features.info()"
      ],
      "metadata": {
        "id": "y78rIKJ0hnHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcdefaults()\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "features['label'].value_counts().plot(kind='barh')\n"
      ],
      "metadata": {
        "id": "BU1hdCvThDzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.corr()"
      ],
      "metadata": {
        "id": "WjI2kIbmhTDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (50,50)\n",
        "sns.heatmap(features.corr(), annot = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PU0oK5J-hZ0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset into a DataFrame\n",
        "data = pd.read_csv('/content/tor_nontor.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.iloc[:, :-1]  # Features (all but the last column)\n",
        "y = data.iloc[:, -1]   # Labels (the last column)\n",
        "non_numeric_columns = X.select_dtypes(exclude=['number']).columns\n",
        "print(\"Non-numeric columns:\", non_numeric_columns)\n",
        "X = pd.get_dummies(X, columns=non_numeric_columns)"
      ],
      "metadata": {
        "id": "BYyLcx3T2u4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "# Initialize your models\n",
        "model = KNeighborsClassifier(n_neighbors=1)\n",
        "model1 = RandomForestClassifier(max_depth=2, random_state=0)\n",
        "model2 = ExtraTreeClassifier(random_state=0)\n",
        "model3 = LogisticRegression(random_state=0)\n",
        "model4 = lgb.LGBMClassifier()\n",
        "\n",
        "# Fit your models\n",
        "model.fit(X, y)\n",
        "model1.fit(X, y)\n",
        "model2.fit(X, y)\n",
        "model3.fit(X, y)\n",
        "model4.fit(X, y)\n",
        "\n",
        "# Cross-validation\n",
        "k_fold = KFold(n_splits=3)\n",
        "scores = cross_val_score(model, X, y, cv=k_fold)\n",
        "scores1 = cross_val_score(model1, X, y, cv=k_fold)\n",
        "scores2 = cross_val_score(model2, X, y, cv=k_fold)\n",
        "scores3 = cross_val_score(model3, X, y, cv=k_fold)\n",
        "scores4 = cross_val_score(model4, X, y, cv=k_fold)\n",
        "\n",
        "# Print scores and their means\n",
        "print(\"Model 1 Scores:\", scores)\n",
        "print(\"Model 1 Mean Score:\", scores.mean())\n",
        "print(\"Model 2 Scores:\", scores1)\n",
        "print(\"Model 2 Mean Score:\", scores1.mean())\n",
        "print(\"Model 3 Scores:\", scores2)\n",
        "print(\"Model 3 Mean Score:\", scores2.mean())\n",
        "print(\"Model 4 Scores:\", scores3)\n",
        "print(\"Model 4 Mean Score:\", scores3.mean())\n",
        "print(\"Model 5 Scores:\", scores4)\n",
        "print(\"Model 5 Mean Score:\", scores4.mean())\n"
      ],
      "metadata": {
        "id": "YAhjJ4Sy267p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset into a DataFrame\n",
        "data = pd.read_csv('/content/tor_nontor.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.iloc[:, :-1]  # Features (all but the last column)\n",
        "y = data.iloc[:, -1]   # Labels (the last column)\n",
        "non_numeric_columns = X.select_dtypes(exclude=['number']).columns\n",
        "X = pd.get_dummies(X, columns=non_numeric_columns)"
      ],
      "metadata": {
        "id": "G-NFKZgZ4XKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('The shape of our features is:', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzavERdZ1FD-",
        "outputId": "486d0b83-0add-4d80-c567-2baab070b97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of our features is: (16144, 984)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the data into training and testing sets\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size = 0.25, random_state = 42)"
      ],
      "metadata": {
        "id": "cy-Zq2uq1vKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training Features Shape:', train_features.shape)\n",
        "print('Training Labels Shape:', train_labels.shape)\n",
        "print('Testing Features Shape:', test_features.shape)\n",
        "print('Testing Labels Shape:', test_labels.shape)"
      ],
      "metadata": {
        "id": "vGNQK6Dj12XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold,cross_val_score\n",
        "import lightgbm as lgb\n",
        "#model = KNeighborsClassifier(n_neighbors = 1)\n",
        "#model.fit(train_features,train_labels)\n",
        "k_fold=KFold(n_splits=3)\n",
        "model=RandomForestClassifier(max_depth=2, random_state=0)\n",
        "model1 = KNeighborsClassifier(n_neighbors=3)\n",
        "model2 = ExtraTreeClassifier(random_state=0)\n",
        "model4= lgb.LGBMClassifier()\n",
        "model.fit(train_features,train_labels)\n",
        "model1.fit(train_features,train_labels)\n",
        "model2.fit(train_features,train_labels)\n",
        "model3=LogisticRegression(random_state=0).fit(train_features,train_labels)\n",
        "model4.fit(train_features,train_labels)\n",
        "scores=cross_val_score(model,train_features,train_labels,cv=k_fold)\n",
        "scores1=cross_val_score(model1,train_features,train_labels,cv=k_fold)\n",
        "scores2=cross_val_score(model2,train_features,train_labels,cv=k_fold)\n",
        "scores3=cross_val_score(model3,train_features,train_labels,cv=k_fold)\n",
        "scores4=cross_val_score(model4,train_features,train_labels,cv=k_fold)\n",
        "print(scores)\n",
        "print(scores.mean())\n",
        "print(scores1)\n",
        "print(scores1.mean())\n",
        "print(scores2)\n",
        "print(scores2.mean())\n",
        "print(scores3)\n",
        "print(scores3.mean())\n",
        "print(scores4)\n",
        "print(scores4.mean())"
      ],
      "metadata": {
        "id": "PPhowQVp1630"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "\n",
        "print()\n",
        "predictions_rf = model.predict(test_features)\n",
        "predictions_knn = model1.predict(test_features)\n",
        "predictions_et = model2.predict(test_features)\n",
        "predictions_lr = model3.predict(test_features)\n",
        "predictions_lightgbm = model4.predict(test_features)\n",
        "# using metrics module for accuracy calculation\n",
        "print(\"Accuracy for Random forest model: \", accuracy_score(test_labels, predictions_rf))\n",
        "print(\"F1 score of the random forest model: \",f1_score(test_labels, predictions_rf, average=\"macro\"))\n",
        "print(\"Precision score of the random forest model: \",precision_score(test_labels, predictions_rf, average=\"macro\"))\n",
        "print(\"Recall score of the random forest model\", recall_score(test_labels, predictions_rf, average=\"macro\"))\n",
        "\n",
        "print(\"Accuracy for KNN model: \", accuracy_score(test_labels, predictions_knn))\n",
        "print(\"F1 score of the KNN model: \",f1_score(test_labels, predictions_knn, average=\"macro\"))\n",
        "print(\"Precision score of the KNN model: \",precision_score(test_labels, predictions_knn, average=\"macro\"))\n",
        "print(\"Recall score of the KNN model\", recall_score(test_labels, predictions_knn, average=\"macro\"))\n",
        "\n",
        "print(\"Accuracy for Eextra trees model: \", accuracy_score(test_labels, predictions_et))\n",
        "print(\"F1 score of the extra trees model: \",f1_score(test_labels, predictions_et, average=\"macro\"))\n",
        "print(\"Precision score of the extra trees model: \",precision_score(test_labels, predictions_et, average=\"macro\"))\n",
        "print(\"Recall score of the extra trees model\", recall_score(test_labels, predictions_et, average=\"macro\"))\n",
        "\n",
        "print(\"Accuracy for logistic regression model: \", accuracy_score(test_labels, predictions_lr))\n",
        "print(\"F1 score of the logistic regression model: \",f1_score(test_labels, predictions_lr, average=\"macro\"))\n",
        "print(\"Precision score of the logistic regression model: \",precision_score(test_labels, predictions_lr, average=\"macro\"))\n",
        "print(\"Recall score of the logistic regression model\", recall_score(test_labels, predictions_lr, average=\"macro\"))\n",
        "\n",
        "print(\"Accuracy for light gbm model: \", accuracy_score(test_labels, predictions_lightgbm))\n",
        "print(\"F1 score of the light gbm model: \",f1_score(test_labels, predictions_lightgbm, average=\"macro\"))\n",
        "print(\"Precision score of the light gbm model: \",precision_score(test_labels, predictions_lightgbm, average=\"macro\"))\n",
        "print(\"Recall score of the light gbm model\", recall_score(test_labels, predictions_lightgbm, average=\"macro\"))\n",
        "#print(test_features)\n",
        "#print(predictions)\n"
      ],
      "metadata": {
        "id": "c0OkP3dG2i2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Read in data and display first 5 rows , xgboost , lightGBM\n",
        "pd.set_option('display.max_columns', None)\n",
        "features_zo = pd.read_csv('tor_nontor.csv')\n",
        "features_zo.head(5)"
      ],
      "metadata": {
        "id": "u2Z96hy57eD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset into a DataFrame\n",
        "data = pd.read_csv('/content/tor_nontor.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X = data.iloc[:, :-1]  # Features (all but the last column)\n",
        "y = data.iloc[:, -1]   # Labels (the last column)\n",
        "non_numeric_columns = X.select_dtypes(exclude=['number']).columns\n",
        "X = pd.get_dummies(X, columns=non_numeric_columns)"
      ],
      "metadata": {
        "id": "R281MnQ37ymA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the data into training and testing sets\n",
        "train_features_xg, test_features_xg, train_labels_xg, test_labels_xg = train_test_split(X, y, test_size = 0.25, random_state = 42)"
      ],
      "metadata": {
        "id": "NNUY9td08QsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "modelxgboost = XGBClassifier()\n",
        "import numpy as np\n",
        "\n",
        "train_labels_xg = np.where(train_labels_xg == 'TOR', 1, 0)\n",
        "modelxgboost.fit(train_features_xg,train_labels_xg)"
      ],
      "metadata": {
        "id": "GvfZr1cx6HSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "\n",
        "print()\n",
        "predictions_xg = modelxgboost.predict(test_features_xg)\n",
        "test_labels_xg = np.where(test_labels_xg == 'TOR', 1, 0)\n",
        "# using metrics module for accuracy calculation\n",
        "print(\"Accuracy of xgboost: \", accuracy_score(test_labels_xg, predictions_xg))\n",
        "print(\"F1 score of the xgboost: \",f1_score(test_labels_xg, predictions_xg, average=\"macro\"))\n",
        "print(\"Precision score of the xgboot: \",precision_score(test_labels_xg, predictions_xg, average=\"macro\"))\n",
        "print(\"Recall score of the xgboost\", recall_score(test_labels_xg, predictions_xg, average=\"macro\"))"
      ],
      "metadata": {
        "id": "EumV60058lJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "\n",
        "model_names = ['Random forest', 'KNN Model', 'Extra trees', 'logistic regression','light gbm model','tabular neural network','xgboost']\n",
        "accuracies = [98.76, 99.92, 99.60, 98.82,99.92, 93.41,99]\n",
        "precision=[98.80,99.92,99.60,98.84,99.60,91,99]\n",
        "# Set bar width and positions\n",
        "bar_width = 0.35\n",
        "bar_positions = np.arange(len(model_names))\n",
        "\n",
        "# Create bar plot\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(bar_positions - bar_width/2, accuracies, bar_width, label='Accuracy')\n",
        "rects3 = ax.bar(bar_positions + bar_width/2, precision, bar_width, label='Precision')\n",
        "# Add labels and legend\n",
        "ax.set_xlabel('Model')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_xticks(bar_positions)\n",
        "ax.set_xticklabels(model_names,rotation=90)\n",
        "ax.set_ylim([90,100])\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cd2rtqvHSk75"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}